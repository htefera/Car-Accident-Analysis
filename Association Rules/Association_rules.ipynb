{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Association rules",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FllH47XeSGP3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Libraries for importing datasets from Google Drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axm_Un_0SzH6"
      },
      "source": [
        "# Configuring Google Drive file loading (run it only once)\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7BYu4SKS1NX"
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1MURORv9iCRNNZtZORoNlv3WtDAYcGYqJ'\n",
        "id = link.split('=')[1]\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile(id)  \n",
        "df = pd.read_csv(id, sep=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kOEe8niTexj"
      },
      "source": [
        "df2 = df.loc[df['Accident_Severity'] == 2].sample(n=2500, random_state=1)\n",
        "#print(df2.shape)\n",
        "df2 = pd.concat([\n",
        "    df2,df.loc[df['Accident_Severity'] == 3].sample(n=2500, random_state=1)\n",
        "],ignore_index=True)\n",
        "#print(df2.shape)\n",
        "\n",
        "df2 = pd.concat([\n",
        "    df2,df.loc[df['Accident_Severity'] == 1]\n",
        "],ignore_index=True)\n",
        "#print(df2.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePSCJIJfH_eC"
      },
      "source": [
        "Downsampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9Z4GHTHvWxy"
      },
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "def create_dataset(dataset):\n",
        "\trandom.seed(time.time())\n",
        "\ttmp = dataset.loc[dataset['Accident_Severity'] == 2].sample(n=2500, random_state = random.randrange(0,1000))\n",
        "\t\n",
        "\ttmp = pd.concat([\n",
        "\t\ttmp,dataset.loc[dataset['Accident_Severity'] == 3].sample(n=2500, random_state = random.randrange(0,1000))\n",
        "\t],ignore_index=True)\n",
        "\t\n",
        "\t\n",
        "\ttmp = pd.concat([\n",
        "\t\ttmp,dataset.loc[dataset['Accident_Severity'] == 1]\n",
        "\t],ignore_index=True)\n",
        "\tprint(tmp.shape)\n",
        "\treturn tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t8E73mVH3Iv"
      },
      "source": [
        "Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh3o44QRueI5"
      },
      "source": [
        "#remove Unknown for Road_Type, Weather_Conditions\n",
        "#remove nan for Road_Surface_Conditions\n",
        "#remove None for Special_Conditions_at_Site, Carriageway_Hazards\n",
        "\n",
        "def remove_nones(l):\n",
        "\tlength1 = len(l)\n",
        "\tj=0\n",
        "\twhile(j < length1):\n",
        "\t\ti = 0\n",
        "\t\tlength = len(l[j])  #list length\n",
        "\t\t#print(row) \n",
        "\t\twhile(i<length):\n",
        "\t\t\tif(l[j][i]=='None' or l[j][i]=='Unknown' or l[j][i] is np.nan or l[j][i]=='nan'):\n",
        "\t\t\t\tl[j].remove (l[j][i])\n",
        "\t\t\t\tlength = length -1  \n",
        "\t\t\t\tcontinue\n",
        "\t\t\ti = i+1\n",
        "\t\tj = j+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-UcX5md79t7"
      },
      "source": [
        "def select_features(l):\n",
        "\tl['Accident_Severity'] = l['Accident_Severity'].map({1: 'Fatal', 2: 'Serious', 3: 'Slight'})\n",
        "\tl['Day_of_Week'] = l['Day_of_Week'].map({1: 'Sun', 2: 'Mon', 3: 'Tus', 4: 'Wed', 5: 'Thu', 6: 'Fri', 7: 'Sat'})\n",
        "\tl['Speed_limit'] = l['Speed_limit'].map({20: 'speed_20', 30: 'speed_30', 40: 'speed_40', 50: 'speed_50', 60: 'speed_60', 70: 'speed_70'})\n",
        "\tl['Urban_or_Rural_Area'] = l['Urban_or_Rural_Area'].map({1: 'Urban', 2: 'Rural'})\t\n",
        "\t#Discretize time\n",
        "\thours = pd.to_datetime(l['Time'], format='%H:%M').dt.hour\n",
        "\tl['Time'] = pd.cut(hours, \n",
        "                    bins=[0,6,12,18,24], \n",
        "                    include_lowest=True, \n",
        "                    labels=['Midnight','Morning','Evening','Night'])\n",
        "\tl['month'] = l['month'].map({1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'})\n",
        "\treturn l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwtm0x8OoPc0"
      },
      "source": [
        "def preprocess_dateset_for_assossiation_rules(dataset):\n",
        "\t# removing useless columns (more than 90% of the data have the same value)\n",
        "\tdataset.drop('Special_Conditions_at_Site', axis = 1,inplace = True)\n",
        "\tdataset.drop('Carriageway_Hazards',axis = 1,inplace = True)\n",
        "\tdataset.drop('Pedestrian_Crossing-Human_Control',axis = 1, inplace= True)\n",
        "\t# removing duplicate of coordinate system\n",
        "\tdataset.drop('Location_Northing_OSGR',axis = 1, inplace = True)\n",
        "\tdataset.drop('Location_Easting_OSGR',axis = 1,inplace = True)\n",
        "\tdataset.drop('Police_Force',axis = 1,inplace = True)\n",
        "\tdataset.drop('Number_of_Vehicles',axis = 1,inplace = True)\n",
        "\tdataset.drop('Number_of_Casualties',axis = 1,inplace = True)\n",
        "\tdataset.drop('Did_Police_Officer_Attend_Scene_of_Accident',axis = 1,inplace = True)\n",
        "\tdataset.drop('Accident_Index',axis = 1,inplace = True)\n",
        "\tdataset['month'] = pd.DatetimeIndex(dataset['Date']).month\n",
        "\tdataset.drop('Date',axis = 1,inplace = True)\n",
        "\tdataset.drop('Junction_Detail',axis = 1,inplace = True)\n",
        " \n",
        "\t#to be used later\n",
        "\tdataset.drop('Longitude',axis = 1,inplace = True)\n",
        "\tdataset.drop('Latitude',axis = 1,inplace = True)\n",
        "\tdataset.drop('1st_Road_Number',axis = 1,inplace = True)\n",
        "\tdataset.drop('2nd_Road_Number',axis = 1,inplace = True)\n",
        "\tdataset.drop('Year',axis = 1,inplace = True)\n",
        "\t\n",
        "\tdataset['Local_Authority_(District)'] = 'local_auth_' + df['Local_Authority_(District)'].astype(str)\n",
        "\tdataset['1st_Road_Class'] = '1st_road_class_' + df['1st_Road_Class'].astype(str)\n",
        "\tdataset['2nd_Road_Class'] = '2nd_road_class_' + df['2nd_Road_Class'].astype(str)\n",
        "\treturn select_features(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eRYakgK4oIl"
      },
      "source": [
        "def preprocess_dateset_for_assossiation_rules2(dataset):\n",
        "  # for every column if it contains just one value (without add Nan to the count) drop it\n",
        "  for col in dataset:\n",
        "    if not dataset[col].value_counts().to_list():\n",
        "      dataset.drop(col, axis = 1,inplace = True)\n",
        "\n",
        "  # removig the index\n",
        "  dataset.drop('Accident_Index',axis = 1, inplace = True)\n",
        "  # removing useless columns (more than 90% of the data have the same value)\n",
        "  dataset.drop('Special_Conditions_at_Site', axis = 1,inplace = True)\n",
        "  dataset.drop('Carriageway_Hazards',axis = 1,inplace = True)\n",
        "  dataset.drop('Pedestrian_Crossing-Human_Control',axis = 1, inplace= True)\n",
        "  # removing duplicate of coordinate system\n",
        "  dataset.drop('Location_Northing_OSGR',axis = 1, inplace = True)\n",
        "  dataset.drop('Location_Easting_OSGR',axis = 1,inplace = True)\n",
        "  dataset.drop('LSOA_of_Accident_Location',axis = 1,inplace = True)\n",
        "  dataset.drop('1st_Road_Number',axis = 1, inplace = True)\n",
        "  dataset.drop('2nd_Road_Number',axis = 1, inplace = True)\n",
        "  dataset.drop('Police_Force',axis = 1, inplace = True)\n",
        "  dataset.drop('Year',axis = 1, inplace = True)\n",
        "\n",
        "  # removing junctioncontrol because it has 40% of nan values\n",
        "  dataset.drop('Junction_Control',axis = 1,inplace = True)\n",
        "  # REMOVE ALSO THE FIRST AND SECOND ORDER STUFF BECAUSE THEY ARE USEFUL ONLY FOR JUNCTIONS\n",
        "\n",
        "  # removing the date because we just use the day of the week\n",
        "  dataset['month'] = pd.DatetimeIndex(dataset['Date']).month\n",
        "  dataset.drop('Date',inplace = True, axis = 1)\n",
        "\n",
        "  # removing 13 nan rows in time, no way to impute it\n",
        "  dataset.dropna(subset = ['Time'],inplace = True)\n",
        "  dataset.dropna(subset = ['Did_Police_Officer_Attend_Scene_of_Accident'],inplace = True)\n",
        "\n",
        "  # merging attribute values\n",
        "  dataset.loc[dataset['Number_of_Vehicles'] > 2,'Number_of_Vehicles'] = '3+'\n",
        "  dataset.loc[dataset['Number_of_Vehicles'] == 2,'Number_of_Vehicles'] = 'two'\n",
        "  dataset.loc[dataset['Number_of_Vehicles'] == 1,'Number_of_Vehicles'] = 'one'\n",
        "  dataset.loc[(dataset['Road_Type'] != 'Single carriageway') & (dataset['Road_Type'] != 'Dual carriageway'),'Road_Type'] = 'Other'\n",
        "  dataset.loc[dataset['Pedestrian_Crossing-Physical_Facilities'] != 'No physical crossing within 50 meters', 'Pedestrian_Crossing-Physical_Facilities'] = 'Physical Crossing present'\n",
        "  dataset.loc[(dataset['Weather_Conditions'] != 'Fine without high winds') & (dataset['Weather_Conditions'] != 'Raining without high winds'), 'Weather_Conditions'] = 'Other'\n",
        "  dataset.loc[(dataset['Road_Surface_Conditions'] != 'Dry') & (dataset['Road_Surface_Conditions'] != 'Wet/Damp'),'Road_Surface_Conditions'] = 'Extreme_condition' \n",
        "  dataset.loc[(dataset['Number_of_Casualties'] != 1) & (dataset['Number_of_Casualties'] != 2),'Number_of_Casualties'] = '3+'\n",
        "  dataset.loc[dataset['Number_of_Casualties'] == 2,'Number_of_Casualties'] = 'two'\n",
        "  dataset.loc[dataset['Number_of_Casualties'] == 1,'Number_of_Casualties'] = 'one' \n",
        "  dataset.loc[(dataset['Light_Conditions'] == 'Daylight: Street light present') | (dataset['Light_Conditions'] == 'Darkness: Street lights present and lit'),'Light_Conditions'] = 'Proper lightning'\n",
        "  dataset.loc[(dataset['Light_Conditions'] == 'Darkness: Street lights present but unlit') | (dataset['Light_Conditions'] == 'Darkeness: No street lighting'),'Light_Conditions'] = 'Insufficient lightning'\n",
        "  dataset.loc[dataset['Light_Conditions'] == 'Darkness: Street lighting unknown','Light_Conditions'] = 'Unknown'\n",
        "  dataset.loc[dataset['Speed_limit'] == 10,'Speed_limit'] = 20\n",
        "\n",
        "  dataset['Local_Authority_(District)'] = 'local_auth_' + df['Local_Authority_(District)'].astype(str)\n",
        "  dataset['1st_Road_Class'] = '1st_road_class_' + df['1st_Road_Class'].astype(str)\n",
        "  dataset['2nd_Road_Class'] = '2nd_road_class_' + df['2nd_Road_Class'].astype(str)\n",
        "  dataset['Number_of_Casualties'] = 'Number_of_Casualties = ' + df['Number_of_Casualties'].astype(str)\n",
        "  dataset['Number_of_Vehicles'] = 'Number_of_Vehicles = ' + df['Number_of_Vehicles'].astype(str)\n",
        "  dataset['Did_Police_Officer_Attend_Scene_of_Accident'] = 'Did_Police_Officer_Attend_Scene_of_Accident = ' + df['Did_Police_Officer_Attend_Scene_of_Accident'].astype(str)\n",
        "  return select_features(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEt_m3fuHygq"
      },
      "source": [
        "MAIN PROGRAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls6O5YaKTyCH"
      },
      "source": [
        "!pip install pyfpgrowth\n",
        "!pip install pygeohash\n",
        "\n",
        "import pyfpgrowth\n",
        "import numpy as np\n",
        "import pygeohash as gh\n",
        "\n",
        "#Transform Latitude and Longitude using geohash\n",
        "df['geohash']=df.apply(lambda x: gh.encode(x.Latitude, x.Longitude, precision=3), axis=1)\n",
        "\n",
        "rules_list = []\n",
        "for iters in range(50):\n",
        "\tprint(\"iteration \"+str(iters)+\" started...\")\n",
        "\tdf2 = create_dataset(df)\n",
        "\tdf3 = preprocess_dateset_for_assossiation_rules(df2)\n",
        "\tdf3 = df3.applymap(str)\n",
        "\t\n",
        "\tdf3_list = df3.values.tolist()\n",
        "\tremove_nones(df3_list)\t\n",
        "\t\n",
        "\tpatterns = pyfpgrowth.find_frequent_patterns(df3_list, 700)\n",
        "\trules = pyfpgrowth.generate_association_rules(patterns, 0.7)\n",
        "\t\n",
        "\trules_list.append(rules)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hCqK7nohyzI"
      },
      "source": [
        "Create an dictionary that contains the association rules and their frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y52SEhR6Qxm6"
      },
      "source": [
        "rules_dict = {}\n",
        "for rules in rules_list:\n",
        "\tprint(\"next_run......................\")\n",
        "\tfor i in rules:\n",
        "\t\tif('Fatal' in rules[i][0]):\n",
        "\t\t\tif(i in rules_dict):\n",
        "\t\t\t\trules_dict[i] += 1\n",
        "\t\t\telse:\n",
        "\t\t\t\trules_dict[i] = 1\n",
        "\t\t\tprint(str(i) + ' -> ' +str(rules[i][0])+' '+str(rules[i][1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQQWaaoHiJ7m"
      },
      "source": [
        "Print the detected association rules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ-dUAWygUX6"
      },
      "source": [
        "for key, value in rules_dict.items() :\n",
        "    print (key, value)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}